{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing necessary packages and libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver \n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from bs4 import Comment\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import requests\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Requests and BeautifulSoup to scrape NBA data for players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(range(2000, 2024))\n",
    "ppg_url = \"https://www.basketball-reference.com/leagues/NBA_{}_ppg.html\"\n",
    "advanced_stats_url = \"https://www.basketball-reference.com/leagues/NBA_{}_advanced.html\"\n",
    "standings_url = \"https://www.basketball-reference.com/leagues/NBA_{}_standings.html\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape MVP awards table from 2000-2023 and save as an HTML file\n",
    "for year in years:\n",
    "    base_url = f\"https://www.basketball-reference.com/awards/awards_{year}.html\"\n",
    "    data = requests.get(base_url)\n",
    "\n",
    "    with open(f'MVP_Data/{year}.html', 'w+', encoding=\"utf-8\") as f:\n",
    "        f.write(data.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting MVP candidates from 2000-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each HTML file, parse out the table, create pandas dataframe\n",
    "mvp_list = []\n",
    "for year in years:\n",
    "    with open(f\"MVP_data/{year}.html\", errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        remove_header = soup.find(\"tr\", class_='over_header').decompose()\n",
    "        mvp_table = soup.find(id='mvp')\n",
    "        mvp_df = pd.read_html(str(mvp_table))[0]\n",
    "        mvp_df['Year'] = year\n",
    "        mvp_list.append(mvp_df)\n",
    "\n",
    "mvp_data = pd.concat(mvp_list)\n",
    "mvp_data.reset_index(drop=True)\n",
    "mvp_data.to_csv(f'MVP_data/mvp_awards.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting ROY candidates from 2000-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "roy_list = []\n",
    "for year in years:\n",
    "    with open(f\"MVP_data/{year}.html\", errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        remove_header = soup.find(\"tr\", class_='over_header').decompose()\n",
    "        roy_table = soup.find(id='roy')\n",
    "        roy_df = pd.read_html(str(roy_table),header=1)[0]\n",
    "        roy_df['Year'] = year\n",
    "        roy_list.append(roy_df)\n",
    "\n",
    "roy_data = pd.concat(roy_list)\n",
    "roy_data.reset_index(drop=True)\n",
    "roy_data.to_csv(f'MVP_data/roy_awards.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting DPOY candidates from 2000-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open each HTML file, parse out the table, create pandas dataframe\n",
    "dpoy_list = []\n",
    "for year in years:\n",
    "    with open(f'MVP_data/{year}.html', errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        soupTables = BeautifulSoup(''.join(soup.find_all(string=lambda text: isinstance(text, Comment) and '<table' in text)))\n",
    "        soupTables.find(\"tr\", class_=\"over_header\").decompose()\n",
    "        dpoy_table = soupTables.find('table', id=\"dpoy\")\n",
    "        dpoy_df = pd.read_html(str(dpoy_table))[0]\n",
    "        dpoy_df['Year'] = year\n",
    "        dpoy_list.append(dpoy_df)\n",
    "\n",
    "dpoy_data = pd.concat(dpoy_list)\n",
    "dpoy_data.reset_index(drop=True)\n",
    "dpoy_data.to_csv('MVP_data/dpoy_awards.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting SMOY candidates from 2000-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoy_list = []\n",
    "for year in years:\n",
    "    with open(f'MVP_data/{year}.html', errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        soupTables = BeautifulSoup(''.join(soup.find_all(string=lambda text: isinstance(text, Comment) and '<table' in text)))\n",
    "        smoy_table = soupTables.find('table', id=\"smoy\")\n",
    "        smoy_df = pd.read_html(str(smoy_table), header=1)[0]\n",
    "        smoy_df['Year'] = year\n",
    "        smoy_list.append(smoy_df)\n",
    "\n",
    "smoy_data = pd.concat(smoy_list)\n",
    "smoy_data.reset_index(drop=True)\n",
    "smoy_data.to_csv('MVP_data/smoy_awards.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Selenium to extract player PPG, Advanced, and Team Record stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats_scraping(years, url, metric=\"\"):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.maximize_window()\n",
    "\n",
    "    for year in years:\n",
    "        driver.get(url.format(year))\n",
    "        time.sleep(5)\n",
    "        pagesource = driver.page_source \n",
    "\n",
    "        with open(f'NBA_Stats/{metric}_{year}.html', 'w', encoding=\"utf-8\") as f:\n",
    "            f.write(pagesource)\n",
    "        print(f\"NBA Season {year} successfully saved.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Season 2000 successfully saved.\n",
      "NBA Season 2001 successfully saved.\n",
      "NBA Season 2002 successfully saved.\n",
      "NBA Season 2003 successfully saved.\n",
      "NBA Season 2004 successfully saved.\n",
      "NBA Season 2005 successfully saved.\n",
      "NBA Season 2006 successfully saved.\n",
      "NBA Season 2007 successfully saved.\n",
      "NBA Season 2008 successfully saved.\n",
      "NBA Season 2009 successfully saved.\n",
      "NBA Season 2010 successfully saved.\n",
      "NBA Season 2011 successfully saved.\n",
      "NBA Season 2012 successfully saved.\n",
      "NBA Season 2013 successfully saved.\n",
      "NBA Season 2014 successfully saved.\n",
      "NBA Season 2015 successfully saved.\n",
      "NBA Season 2016 successfully saved.\n",
      "NBA Season 2017 successfully saved.\n",
      "NBA Season 2018 successfully saved.\n",
      "NBA Season 2019 successfully saved.\n",
      "NBA Season 2020 successfully saved.\n",
      "NBA Season 2021 successfully saved.\n",
      "NBA Season 2022 successfully saved.\n",
      "NBA Season 2023 successfully saved.\n"
     ]
    }
   ],
   "source": [
    "stats_scraping(years, ppg_url, \"ppg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Season 2000 successfully saved.\n",
      "NBA Season 2001 successfully saved.\n",
      "NBA Season 2002 successfully saved.\n",
      "NBA Season 2003 successfully saved.\n",
      "NBA Season 2004 successfully saved.\n",
      "NBA Season 2005 successfully saved.\n",
      "NBA Season 2006 successfully saved.\n",
      "NBA Season 2007 successfully saved.\n",
      "NBA Season 2008 successfully saved.\n",
      "NBA Season 2009 successfully saved.\n",
      "NBA Season 2010 successfully saved.\n",
      "NBA Season 2011 successfully saved.\n",
      "NBA Season 2012 successfully saved.\n",
      "NBA Season 2013 successfully saved.\n",
      "NBA Season 2014 successfully saved.\n",
      "NBA Season 2015 successfully saved.\n",
      "NBA Season 2016 successfully saved.\n",
      "NBA Season 2017 successfully saved.\n",
      "NBA Season 2018 successfully saved.\n",
      "NBA Season 2019 successfully saved.\n",
      "NBA Season 2020 successfully saved.\n",
      "NBA Season 2021 successfully saved.\n",
      "NBA Season 2022 successfully saved.\n",
      "NBA Season 2023 successfully saved.\n"
     ]
    }
   ],
   "source": [
    "stats_scraping(years, advanced_stats_url, \"advanced_stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NBA Season 2000 successfully saved.\n",
      "NBA Season 2001 successfully saved.\n",
      "NBA Season 2002 successfully saved.\n",
      "NBA Season 2003 successfully saved.\n",
      "NBA Season 2004 successfully saved.\n",
      "NBA Season 2005 successfully saved.\n",
      "NBA Season 2006 successfully saved.\n",
      "NBA Season 2007 successfully saved.\n",
      "NBA Season 2008 successfully saved.\n",
      "NBA Season 2009 successfully saved.\n",
      "NBA Season 2010 successfully saved.\n",
      "NBA Season 2011 successfully saved.\n",
      "NBA Season 2012 successfully saved.\n",
      "NBA Season 2013 successfully saved.\n",
      "NBA Season 2014 successfully saved.\n",
      "NBA Season 2015 successfully saved.\n",
      "NBA Season 2016 successfully saved.\n",
      "NBA Season 2017 successfully saved.\n",
      "NBA Season 2018 successfully saved.\n",
      "NBA Season 2019 successfully saved.\n",
      "NBA Season 2020 successfully saved.\n",
      "NBA Season 2021 successfully saved.\n",
      "NBA Season 2022 successfully saved.\n",
      "NBA Season 2023 successfully saved.\n"
     ]
    }
   ],
   "source": [
    "stats_scraping(years, standings_url, \"team_standings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppg_list = []\n",
    "for year in years:\n",
    "    with open(f'NBA_Stats/ppg_{year}.html', 'r', errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        for item in soup.find_all(\"tr\", class_=\"thead\"):\n",
    "            item.decompose()\n",
    "        ppg_table = soup.find(id=\"per_game_stats\")\n",
    "        ppg_df = pd.read_html(str(ppg_table))[0]\n",
    "        ppg_df['Year'] = year\n",
    "        ppg_list.append(ppg_df)\n",
    "\n",
    "ppg_data = pd.concat(ppg_list)\n",
    "ppg_data.reset_index(drop=True)\n",
    "ppg_data.to_csv('NBA_Stats/ppg_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "advanced_stats_list = []\n",
    "for year in years:\n",
    "    with open(f'NBA_Stats/advanced_stats_{year}.html', 'r', errors=\"ignore\") as f:\n",
    "        page = f.read()\n",
    "\n",
    "        soup = BeautifulSoup(page, \"html.parser\")\n",
    "        for item in soup.find_all(\"tr\", class_=\"thead\"):\n",
    "            item.decompose()\n",
    "        advanced_stats_table = soup.find(id=\"advanced_stats\")\n",
    "        advanced_stats_df = pd.read_html(str(advanced_stats_table))[0]\n",
    "        advanced_stats_df['Year'] = year\n",
    "        advanced_stats_list.append(advanced_stats_df)\n",
    "\n",
    "advanced_stats_data = pd.concat(advanced_stats_list)\n",
    "advanced_stats_data.reset_index(drop=True)\n",
    "advanced_stats_data.to_csv('NBA_Stats/advanced_stats_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'NBA_Stats/advanced_stats_2000.html', 'r', errors=\"ignore\") as f:\n",
    "    page = f.read()\n",
    "\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
